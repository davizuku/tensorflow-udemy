{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "- Automatically determine the ideal behavior within a specific context, in order to maximize its performance\n",
    "- Simple reward feedback to reinforce good behavior.\n",
    "- Main elements:\n",
    "  - Agent: the model we are programming\n",
    "  - Observations + Rewards: the input to the agent\n",
    "  - Environment: problem we are trying to solve, often a game. Represented as an array. OpenAI Gym offers a good starting point.\n",
    "  - Action: decisions of the model\n",
    "\n",
    "## Useful resources: \n",
    "\n",
    "Wikipedia: https://en.wikipedia.org/wiki/Reinforcement_learning\n",
    "\n",
    "Keras: https://github.com/matthiasplappert/keras-rl\n",
    "\n",
    "RL Algorithm Repository: https://github.com/dennybritz/reinforcement-learning\n",
    "\n",
    "Berkeley course: http://rll.berkeley.edu/deeprlcourse/\n",
    "\n",
    "Articles: https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Official documentation: gym.openai.com\n",
    "\n",
    "Available environments: http://gym.openai.com/envs/#classic_control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "- Use editor or IDE and command line.\n",
    "- Code in .py, do not use Notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It WORKS\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "print(\"It WORKS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gym Elements\n",
    "\n",
    "A the `step` function returns the following values: \n",
    "\n",
    "- observation: The state of the environment.\n",
    "- reward: The amount of reward achieved by the previous action. The number may vary depending on the environment. \n",
    "- done: Boolean indicating whether the environment needs to be reset.\n",
    "- info: Dictionary with diagnostic information, used for debugging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Initial Observation\n",
      "[ 0.02038799  0.01437287  0.02857049  0.01362293]\n",
      "\n",
      "\n",
      "Performed One Random Action\n",
      "observation\n",
      "[ 0.02067544  0.20907368  0.02884295 -0.26991052]\n",
      "reward\n",
      "1.0\n",
      "done\n",
      "False\n",
      "info\n",
      "{}\n",
      "\n",
      "\n",
      "Performed One Random Action\n",
      "observation\n",
      "[ 0.02485692  0.40377241  0.02344473 -0.5533585 ]\n",
      "reward\n",
      "1.0\n",
      "done\n",
      "False\n",
      "info\n",
      "{}\n",
      "\n",
      "\n",
      "Performed One Random Action\n",
      "observation\n",
      "[ 0.03293237  0.59855742  0.01237756 -0.8385636 ]\n",
      "reward\n",
      "1.0\n",
      "done\n",
      "False\n",
      "info\n",
      "{}\n",
      "\n",
      "\n",
      "Performed One Random Action\n",
      "observation\n",
      "[ 0.04490351  0.79350819 -0.00439371 -1.12732843]\n",
      "reward\n",
      "1.0\n",
      "done\n",
      "False\n",
      "info\n",
      "{}\n",
      "\n",
      "\n",
      "Performed One Random Action\n",
      "observation\n",
      "[ 0.06077368  0.59844407 -0.02694028 -0.83602684]\n",
      "reward\n",
      "1.0\n",
      "done\n",
      "False\n",
      "info\n",
      "{}\n",
      "\n",
      "\n",
      "Performed One Random Action\n",
      "observation\n",
      "[ 0.07274256  0.79392345 -0.04366081 -1.13705899]\n",
      "reward\n",
      "1.0\n",
      "done\n",
      "False\n",
      "info\n",
      "{}\n",
      "\n",
      "\n",
      "Performed One Random Action\n",
      "observation\n",
      "[ 0.08862103  0.59939893 -0.06640199 -0.85838247]\n",
      "reward\n",
      "1.0\n",
      "done\n",
      "False\n",
      "info\n",
      "{}\n",
      "\n",
      "\n",
      "Performed One Random Action\n",
      "observation\n",
      "[ 0.10060901  0.40524133 -0.08356964 -0.58729575]\n",
      "reward\n",
      "1.0\n",
      "done\n",
      "False\n",
      "info\n",
      "{}\n",
      "\n",
      "\n",
      "Performed One Random Action\n",
      "observation\n",
      "[ 0.10871383  0.60142811 -0.09531556 -0.90508979]\n",
      "reward\n",
      "1.0\n",
      "done\n",
      "False\n",
      "info\n",
      "{}\n",
      "\n",
      "\n",
      "Performed One Random Action\n",
      "observation\n",
      "[ 0.1207424   0.79770262 -0.11341735 -1.22614555]\n",
      "reward\n",
      "1.0\n",
      "done\n",
      "False\n",
      "info\n",
      "{}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print('Initial Observation')\n",
    "observation = env.reset()\n",
    "print(observation)\n",
    "print('\\n')\n",
    "for _ in range(10):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    print(\"Performed One Random Action\")\n",
    "    print('observation')\n",
    "    print(observation)\n",
    "    print('reward')\n",
    "    print(reward)\n",
    "    print('done')\n",
    "    print(done)\n",
    "    print('info')\n",
    "    print(info)\n",
    "    print('\\n')\n",
    "    time.sleep(0.5)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "# Documentation: https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py#L40\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(4,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym \n",
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset()\n",
    "\n",
    "for t in range(1000):\n",
    "    env.render()\n",
    "    cart_pos, cart_vel, pole_ang, ang_vel = observation\n",
    "    action = int(pole_ang > 0)\n",
    "    observation, rewared, done, info = env.step(action)\n",
    "    if (done): \n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network Game\n",
    "\n",
    "- Create a Neural Network that returns the probability of going left, given the four numbers of our observation.\n",
    "- With the probability, we'll choose randomly which action to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_inputs = 4\n",
    "num_hidden = 4\n",
    "num_outputs = 1 # Probability to go left 1-left = right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initializer = tf.contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, num_inputs])\n",
    "hidden_layer_one = tf.layers.dense(\n",
    "    X, \n",
    "    num_hidden, \n",
    "    activation=tf.nn.relu, \n",
    "    kernel_initializer=initializer\n",
    ")\n",
    "hidden_layer_two = tf.layers.dense(\n",
    "    hidden_layer_one, \n",
    "    num_hidden, \n",
    "    activation=tf.nn.relu, \n",
    "    kernel_initializer=initializer\n",
    ")\n",
    "output_layer = tf.layers.dense(\n",
    "    hidden_layer_two, \n",
    "    num_outputs,\n",
    "    activation=tf.nn.sigmoid,\n",
    "    kernel_initializer=initializer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probabilities = tf.concat(axis=1, values=[output_layer, 1-output_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action = tf.multinomial(probabilities, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Done after 12 steps\n",
      "Done after 13 steps\n",
      "Done after 30 steps\n",
      "Done after 10 steps\n",
      "Done after 43 steps\n",
      "Done after 15 steps\n",
      "Done after 15 steps\n",
      "Done after 13 steps\n",
      "Done after 17 steps\n",
      "Done after 8 steps\n",
      "Done after 15 steps\n",
      "Done after 10 steps\n",
      "Done after 12 steps\n",
      "Done after 16 steps\n",
      "Done after 29 steps\n",
      "Done after 23 steps\n",
      "Done after 15 steps\n",
      "Done after 38 steps\n",
      "Done after 25 steps\n",
      "Done after 22 steps\n",
      "Done after 9 steps\n",
      "Done after 13 steps\n",
      "Done after 11 steps\n",
      "Done after 10 steps\n",
      "Done after 21 steps\n",
      "Done after 28 steps\n",
      "Done after 13 steps\n",
      "Done after 19 steps\n",
      "Done after 16 steps\n",
      "Done after 14 steps\n",
      "Done after 11 steps\n",
      "Done after 8 steps\n",
      "Done after 23 steps\n",
      "Done after 10 steps\n",
      "Done after 12 steps\n",
      "Done after 12 steps\n",
      "Done after 11 steps\n",
      "Done after 13 steps\n",
      "Done after 11 steps\n",
      "Done after 17 steps\n",
      "Done after 12 steps\n",
      "Done after 20 steps\n",
      "Done after 40 steps\n",
      "Done after 27 steps\n",
      "Done after 9 steps\n",
      "Done after 12 steps\n",
      "Done after 10 steps\n",
      "Done after 8 steps\n",
      "Done after 16 steps\n",
      "Done after 11 steps\n",
      "Done after 14 steps\n",
      "Done after 15 steps\n",
      "Done after 14 steps\n",
      "Done after 14 steps\n",
      "Done after 11 steps\n",
      "Done after 26 steps\n",
      "Done after 19 steps\n",
      "Done after 12 steps\n",
      "Done after 14 steps\n",
      "Done after 8 steps\n",
      "Done after 9 steps\n",
      "Done after 21 steps\n",
      "Done after 14 steps\n",
      "Done after 8 steps\n",
      "Done after 10 steps\n",
      "Done after 15 steps\n",
      "Done after 8 steps\n",
      "Done after 13 steps\n",
      "Done after 16 steps\n",
      "Done after 10 steps\n",
      "Done after 9 steps\n",
      "Done after 16 steps\n",
      "Done after 12 steps\n",
      "Done after 8 steps\n",
      "Done after 14 steps\n",
      "Done after 25 steps\n",
      "Done after 15 steps\n",
      "Done after 15 steps\n",
      "Done after 17 steps\n",
      "Done after 22 steps\n",
      "Done after 16 steps\n",
      "Done after 12 steps\n",
      "Done after 19 steps\n",
      "Done after 16 steps\n",
      "Done after 8 steps\n",
      "Done after 12 steps\n",
      "Done after 22 steps\n",
      "Done after 18 steps\n",
      "Done after 15 steps\n",
      "Done after 39 steps\n",
      "Done after 10 steps\n",
      "Done after 11 steps\n",
      "Done after 16 steps\n",
      "Done after 9 steps\n",
      "Done after 9 steps\n",
      "Done after 17 steps\n",
      "Done after 32 steps\n",
      "Done after 21 steps\n",
      "Done after 10 steps\n",
      "Done after 21 steps\n",
      "Done after 15 steps\n",
      "Done after 12 steps\n",
      "Done after 9 steps\n",
      "Done after 13 steps\n",
      "Done after 9 steps\n",
      "Done after 10 steps\n",
      "Done after 13 steps\n",
      "Done after 40 steps\n",
      "Done after 29 steps\n",
      "Done after 16 steps\n",
      "Done after 8 steps\n",
      "Done after 19 steps\n",
      "Done after 16 steps\n",
      "Done after 10 steps\n",
      "Done after 12 steps\n",
      "Done after 11 steps\n",
      "Done after 15 steps\n",
      "Done after 28 steps\n",
      "Done after 11 steps\n",
      "Done after 17 steps\n",
      "Done after 19 steps\n",
      "Done after 10 steps\n",
      "Done after 28 steps\n",
      "Done after 13 steps\n",
      "Done after 10 steps\n",
      "Done after 14 steps\n",
      "Done after 25 steps\n",
      "Done after 12 steps\n",
      "Done after 10 steps\n",
      "Done after 11 steps\n",
      "Done after 14 steps\n",
      "Done after 28 steps\n",
      "Done after 13 steps\n",
      "Done after 12 steps\n",
      "Done after 19 steps\n",
      "Done after 13 steps\n",
      "Done after 21 steps\n",
      "Done after 14 steps\n",
      "Done after 15 steps\n",
      "Done after 21 steps\n",
      "Done after 16 steps\n",
      "Done after 16 steps\n",
      "Done after 17 steps\n",
      "Done after 10 steps\n",
      "Done after 12 steps\n",
      "Done after 18 steps\n",
      "Done after 16 steps\n",
      "Done after 17 steps\n",
      "Done after 8 steps\n",
      "Done after 14 steps\n",
      "Done after 9 steps\n",
      "Done after 7 steps\n",
      "Done after 8 steps\n",
      "Done after 26 steps\n",
      "Done after 10 steps\n",
      "Done after 16 steps\n",
      "Done after 11 steps\n",
      "Done after 12 steps\n",
      "Done after 19 steps\n",
      "Done after 10 steps\n",
      "Done after 23 steps\n",
      "Done after 11 steps\n",
      "Done after 10 steps\n",
      "Done after 17 steps\n",
      "Done after 20 steps\n",
      "Done after 11 steps\n",
      "Done after 18 steps\n",
      "Done after 24 steps\n",
      "Done after 12 steps\n",
      "Done after 13 steps\n",
      "Done after 21 steps\n",
      "Done after 15 steps\n",
      "Done after 22 steps\n",
      "Done after 15 steps\n",
      "Done after 14 steps\n",
      "Done after 15 steps\n",
      "Done after 14 steps\n",
      "Done after 19 steps\n",
      "Done after 8 steps\n",
      "Done after 20 steps\n",
      "Done after 14 steps\n",
      "Done after 15 steps\n",
      "Done after 12 steps\n",
      "Done after 9 steps\n",
      "Done after 19 steps\n",
      "Done after 19 steps\n",
      "Done after 13 steps\n",
      "Done after 11 steps\n",
      "Done after 12 steps\n",
      "Done after 18 steps\n",
      "Done after 25 steps\n",
      "Done after 8 steps\n",
      "Done after 17 steps\n",
      "Done after 27 steps\n",
      "Done after 33 steps\n",
      "Done after 10 steps\n",
      "Done after 16 steps\n",
      "Done after 10 steps\n",
      "Done after 14 steps\n",
      "Done after 11 steps\n",
      "Done after 13 steps\n",
      "Done after 23 steps\n",
      "Done after 18 steps\n",
      "Done after 14 steps\n",
      "Done after 33 steps\n",
      "Done after 11 steps\n",
      "Done after 11 steps\n",
      "Done after 14 steps\n",
      "Done after 20 steps\n",
      "Done after 11 steps\n",
      "Done after 15 steps\n",
      "Done after 21 steps\n",
      "Done after 18 steps\n",
      "Done after 10 steps\n",
      "Done after 14 steps\n",
      "Done after 14 steps\n",
      "Done after 9 steps\n",
      "Done after 9 steps\n",
      "Done after 13 steps\n",
      "Done after 23 steps\n",
      "Done after 11 steps\n",
      "Done after 21 steps\n",
      "Done after 11 steps\n",
      "Done after 9 steps\n",
      "Done after 21 steps\n",
      "Done after 9 steps\n",
      "Done after 21 steps\n",
      "Done after 26 steps\n",
      "Done after 13 steps\n",
      "Done after 14 steps\n",
      "Done after 35 steps\n",
      "Done after 23 steps\n",
      "Done after 16 steps\n",
      "Done after 10 steps\n",
      "Done after 23 steps\n",
      "Done after 12 steps\n",
      "Done after 9 steps\n",
      "Done after 11 steps\n",
      "Done after 12 steps\n",
      "Done after 48 steps\n",
      "Done after 11 steps\n",
      "Done after 15 steps\n",
      "Done after 12 steps\n",
      "Done after 9 steps\n",
      "Done after 7 steps\n",
      "Done after 11 steps\n",
      "Done after 15 steps\n",
      "Done after 25 steps\n",
      "Done after 15 steps\n",
      "Done after 11 steps\n",
      "After 250 episodes, average steps per game was 15.708\n"
     ]
    }
   ],
   "source": [
    "epi = 250\n",
    "step_limit = 500\n",
    "env = gym.make('CartPole-v0')\n",
    "avg_steps = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for i_episode in range(epi):\n",
    "        obs = env.reset()\n",
    "        \n",
    "        for step in range(step_limit):\n",
    "            action_val = action.eval(feed_dict={X: obs.reshape(1, num_inputs)})\n",
    "            obs, reward, done, info = env.step(action_val[0][0]) # 0 or 1\n",
    "            \n",
    "            if done:\n",
    "                avg_steps.append(step)\n",
    "                print(\"Done after {} steps\".format(step))\n",
    "                break\n",
    "                \n",
    "print(\"After {} episodes, average steps per game was {}\".format(epi, np.mean(avg_steps)))\n",
    "env.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy gradients\n",
    "\n",
    "- Previous algorithm only considered last action and its consequences\n",
    "- Assigment of credit problem: Which actions should be credited when the agent gets rewarded at time `t`?\n",
    "- Solution: Discount rate, to be chosen: 0.95 - 0.99. Multiply the rewards by this discount rate at every time point with a power proportional to the time point.\n",
    "\n",
    "Basic algorithm:\n",
    "- Calculate the score of an action based on future actions\n",
    "- Normalize the score by substracting the mean and dividing by std deviation.\n",
    "\n",
    "- Neural Network plays several episodes\n",
    "- The optimizer will calculate the gradients, instead of calling minimize\n",
    "- Compute each action's discounted and normalized score.\n",
    "- Multiply the gradient vector by the action's score\n",
    "- Negative scores will create opposite gradients when multiplied\n",
    "- Calculate mean of the resulting gradient vector for Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_inputs = 4\n",
    "num_hidden = 4\n",
    "num_outputs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initializer = tf.contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, num_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_layer = tf.layers.dense(\n",
    "    X, \n",
    "    num_hidden, \n",
    "    activation=tf.nn.elu, \n",
    "    kernel_initializer=initializer\n",
    ")\n",
    "logits = tf.layers.dense(\n",
    "    hidden_layer, \n",
    "    num_outputs\n",
    ")\n",
    "outputs = tf.nn.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probabilities = tf.concat(axis=1, values=[outputs, 1-outputs])\n",
    "action = tf.multinomial(probabilities, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = 1.0 - tf.to_float(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    labels=y, \n",
    "    logits=logits\n",
    ")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradients_and_variables = optimizer.compute_gradients(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-1e7bbe8e237a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgradients_and_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mgradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mgradient_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mgradient_placeholders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mgrads_and_vars_feed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_placeholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_shape'"
     ]
    }
   ],
   "source": [
    "gradients = []\n",
    "gradient_placeholders = []\n",
    "grads_and_vars_feed = []\n",
    "\n",
    "for gradient, variable in gradients_and_variables:\n",
    "    gradients.append(gradient)\n",
    "    gradient_placeholder = tf.placeholder(tf.float32, shape=gradient.get_shape())\n",
    "    gradient_placeholders.append(gradient_placeholder)\n",
    "    grads_and_vars_feed.append((gradient_placeholder, variable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No variables provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-af0e5cf5727d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars_feed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Make sure repeat iteration works.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No variables provided.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m     \u001b[0mconverted_grads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No variables provided."
     ]
    }
   ],
   "source": [
    "training_op = optimizer.apply_gradients(grads_and_vars_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def helper_discount_rewards(rewards, discount_rate):\n",
    "    '''\n",
    "    Takes in rewards and applies discount rate\n",
    "    '''\n",
    "    discounted_rewards = np.zeros(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    '''\n",
    "    Takes in all rewards, applies helper_discount function and then normalizes\n",
    "    using mean and std.\n",
    "    '''\n",
    "    all_discounted_rewards = []\n",
    "    for rewards in all_rewards:\n",
    "        all_discounted_rewards.append(\n",
    "            helper_discount_rewards(rewards,discount_rate)\n",
    "        )\n",
    "\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/reward_std for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "On iteration 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_game_rounds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-729f49a993fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mall_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mgame\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_game_rounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mcurrent_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_game_rounds' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "num_game_rounds = 10\n",
    "max_game_steps = 1000\n",
    "num_iterations = 750 \n",
    "discount_rate = 0.9\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for iteration in range(num_iterations):\n",
    "        print(\"On iteration {}\".format(iteration))\n",
    "        \n",
    "        all_rewards = []\n",
    "        all_gradients = []\n",
    "        \n",
    "        for game in range(num_game_rounds):\n",
    "            \n",
    "            current_rewards = []\n",
    "            current_gradients = []\n",
    "            observation = env.reset()\n",
    "            \n",
    "            for step in range(max_game_steps):\n",
    "                \n",
    "                action_val, gradients_val = sess.run(\n",
    "                    [ action, gradients ], \n",
    "                    feed_dict={X:observations.reshape(1, num_inputs)}\n",
    "                )\n",
    "                observations, reward, done, info = env.step(action_val[0][0])\n",
    "                current_rewards.append(reward)\n",
    "                current_gradients.append(gradients_val)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            all_rewards.append(current_rewards)\n",
    "            all_gradients.append(current_gradients)\n",
    "            \n",
    "        all_rewards = discount_and_normalize_rewards(all_rewards, discount_rate)\n",
    "        feed_dict = {}\n",
    "        \n",
    "        for var_index, gradient_placeholder in enumerate(gradient_placeholders):\n",
    "            mean_gradients = np.mean(\n",
    "                [reward * all_gradients[game_index][step][var_index]\n",
    "                for game_index, rewards in enumerate(all_rewards)\n",
    "                    for step, reward in enumerate(rewards)], axis=0\n",
    "            )\n",
    "            feed_dict[gradient_placeholder] = mean_gradients\n",
    "            \n",
    "        sess.run(training_op, feed_dict)\n",
    "        \n",
    "        print('SAVING GRAPH AND SESSION')\n",
    "        meta_graph_def = tf.train.export_meta_graph(filename='./models/davizuku-policy-model.meta')\n",
    "        saver.save(sess, './models/davizuku-policy-model')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "observations = env.reset()\n",
    "with tf.Session() as sess:\n",
    "    # https://www.tensorflow.org/api_guides/python/meta_graph\n",
    "    new_saver = tf.train.import_meta_graph('/models/davizuku-policy-model.meta')\n",
    "    new_saver.restore(sess,'/models/davizuku-policy-model')\n",
    "\n",
    "    for x in range(500):\n",
    "        env.render()\n",
    "        action_val, gradients_val = sess.run([action, gradients], feed_dict={X: observations.reshape(1, num_inputs)})\n",
    "        observations, reward, done, info = env.step(action_val[0][0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
